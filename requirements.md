# Выполнение задания

Задание выполнено с помощью следующих технологий:
* Python;
* TSQL;
* Postgre;
* Airflow;
* Docker.

Для запуска учебной инфраструктуры клонируйте к себе на ПК репозиторий:

    git clone https://github.com/xxxRichiexxx/Neoflex_1

Далее выполните из папки "Neoflex_1" следующую комманду:

    docker-compose up -d --build

После запуска данной комманды будет развернуто несколько контейнеров: 

* Airflow с базой метаданных Postgres;
* КХД на базе Postgres.

Папки "Neoflex_1/Data" и "Neoflex_1/dags" смонтированы в контейнер Airflow, т.е все скрипты, размещенные в папке dags, доступны для Airflow, все файлы с данными для загрузки в папке data доступны для Airflow.

Подключитесь к контейнеру DWH с помощью Debeaver:



Примените в базе данных bank следующие скрипты "Neoflex_1/DDL.sql".


Зайдите в Airflow по следующей ссылке:

    http://localhost/home

Введите логин и пароль:

    login: airflow
    password: airflow

Создайте в Airflow подключение к DWH:


Запустите DAGи.


При выполнении задания использовался следующий подход:

* Все исходные данные из формата csv сначала загружаются в слой raw без изменений. Данный слой разработан таким образом, чтобы минимизировать проблемы при загрузке данных в хранилище и избежать повторных запросов данных в источнике;
* Далее данные из слоя raw загружаются в слой ODS и слой DDS. Два слоя созданы для того, чтобы продемонстрировать два решения поставленной задачи. В первом решении данные загружаются в хранилище с предваримтельной очисткой, преобразованием форматов, но без создания связей в целевых таблицах. Для демонстрации данного решения создан слой ODS. Во втором решении данные загружаются в хранилище с предварительной очисткой, преобразованием форматов и созданием связей между целевыми таблицами. Данное решение продемонстрированно в слое DDS.

Для загрузки данных в слой raw испотзуются динамические DAG (см. dags/to_raw_dag.py).
Данный скрипт генерирует DAGи на основе yaml-файла (см. dags/to_raw_dag_config.yaml): для каждого csv-файла свой DAG.

После окончания загрузки данных в raw-слой начинают работать DAGи, загружающие данные в слои ODS и DDS (данные DAGи ожидают окончания загрузки данных в raw с помощью ExternalTaskSensor).

Работа дагов логируется средствами Airflow и по заданию промсходлит запись логов в специальную таблицу LOGS.DAG_LOGS.

СКРИН с логами

Кроме того, данные, не загруженные в слой DDS (из-за невыполнения ограничений) копируются в слой REJECTED_DATA.

Скрин с DDS.